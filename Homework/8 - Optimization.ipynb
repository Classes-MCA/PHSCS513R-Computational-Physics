{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Washboard Potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"Washboard potentials\" are 1-D potential energy functions with many local minima. They arise in the theory of supercondicting Josephson junctions, the motion of defects in crystals, and many other applications. Consider the following washbarod potential\n",
    "\n",
    "$$V(r) = A_1 \\cos(r) + A_2 \\cos(2r) - F r$$\n",
    "\n",
    "with A_1 = 5, A_2 = 1, and F initially set to 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot $V(r)$ over $(-10,10)$. Numerically find the local maximum of $V$ near zero and the ocal minimum of $V$ to the left (negative side) of zero. What is the potential energy barrier for moving from one well to the next in this potential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Usually finding hte minimum is only a first step. Often, one wants to explore how the minimum moves and disappears. Increase the external tilting field $F$, roughly estimate the field $F_c$ at which the barrier diappears. Also, estimate the location $r_c$ at this field where the potential minimum and maximum merge. (This is an example of a saddle-node bifurication.) Give the criterion on the first derivative and the second derivative of $V(r)$ at $F_c$ and $r_c$. Using these two equations, numerically use a root-finding routine to olocate the saddle-node bifurication $F_c$ and $r_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Sloppy Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Optimization problems that arise in data fitting are often extremely ill-conditioned (a phenomenon sometimes called \"sloppiness\"). A classically ill-conditioned fitting problem involves a sum of oexponentials to data. Consider the function\n",
    "\n",
    "$$y(t) = \\frac{1}{N} \\sum_{n = 0}^{N-1} \\lambda_n e^{\\lambda_n t}$$\n",
    "\n",
    "Now suppose you don't know the parameters $\\lambda_n$. Can you reconstruct them by fitting to experimental data $y_0(t)$?\n",
    "\n",
    "Consider the case of $N = 2$. Suppose the actual parameters are $\\lambda_0 = 2$ and $\\lambda_1 = 3$ (so the experiment has data $y_0(t) = e^{-2 t} + \\frac{3}{2}e^{-3 t}$) and we try to monomize the least squared error in the fits C:\n",
    "\n",
    "$$C(\\lambda) = \\int_0^\\infty (y(t) - y_0(t))^2 dt$$\n",
    "\n",
    "You can check thaht this integral evaluates to\n",
    "\n",
    "$$C(\\lambda) = -\\frac{51}{40} + \\frac{\\lambda_0}{8} - \\frac{\\lambda_0}{2 + \\lambda_0} - \\frac{3 \\lambda_0}{2(3 + \\lambda_0)} + \\frac{\\lambda_1}{8} + \\frac{2}{2 + \\lambda_1} + \\frac{9}{2(3 + \\lambda_1)} + \\frac{\\lambda_0 \\lambda_1}{2(\\lambda_0 + \\lambda_1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Draw a contour plot of $C$ in the square $1.5 < \\lambda_n < 3.5$ with enough contours (perhaps non-equally spaced) so that the two minima can be distinguished. (You'll also need a fiarly fine grid of points.) You should see that inferring the two rate constants separately is challenging. This is because the two exponentials have similar shapes, so increasing one decay rate and decreasing the other can almost perfectly compensate for one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume the two parameters are equal $(\\lambda = \\lambda_0 = \\lambda_1)$, minimize the cost to find the optimal choice for $\\lambda$. Where is this point on the contour plot? Plot $y_0(t)$ and $y(t)$ with thtis single-exponent best fit on the same graph, over $0 < t < 2$. Do you agree that it would be difficult to distinguish these two fits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This problem becomes much more severe in high dimensions. The banana-shaped ellipses on your contour plot can become needle-like, with aspect ratios of more than a thousand to one (about the same as a human hair). Foollowing these thin paths down to the true minima can be a challenge for multi-dimensional minimization programs. Find a method for storing and plotting the locations visited by your minimization routine (ie. values $(\\lambda_0,\\lambda_1)$ at which it evaluates $C$ while searching for the minimum). Starting from $\\lambda_0 = 1, \\lambda_1 = 4$, minimize the cost using as many methods as is convenient within your programming environment, eg. Nelder-Mead, Powell, Newton, Quasi-Newton, conjugate gradient, Levenberge-Margquardt (aka nonlinear least squares). Try to use at least one that does not demand erivatives of the function. Plot the evaluation pooints on top of the contour plot of the cost for each method. Compare the number of function evaluations needed for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Convex Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider a vector in $x \\in R^n$. The I-p norms are a family of vector norms parameterized by p:\n",
    "\n",
    "$$||x||_p = \\left( \\sum_{k = 0}^{n-1} |x_p|^p \\right)^{\\frac{1}{p}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Show that $||x||_p$ is a convex function for any $p \\geq 1$. Hint, you may find the Minkowski inequality helpful:\n",
    "\n",
    "$$||x + y||_p \\leq ||x||_p + ||y||_p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The I-p norm defines a valid vector norm for any $p \\geq 1$. for $p < 1$, it no longer defines a norm, but sometimes it is useful to think about hte function for $p < 1$. Show that the \"I-0\" norm counts the numebr of non-zero components of the vector. Show that the \"I-0\" norm is not convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In your environment, fin a convex optimization DXL. What are soome of hte algorithms that are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Solve the following optimization problem:\n",
    "\n",
    "$$\\textrm{min}_x ||J(x - x_0)||_2^2 + \\lambda||x||_1$$\n",
    "\n",
    "$$x_i \\geq 0$$\n",
    "\n",
    "for the $J$ and $x_0$ in the attached file (get zip file online).\n",
    "\n",
    "$||\\cdot||_p$ denotes the p-norm and $\\lambda = 0, 10^{-8},10^{-4},10^{-2},1$ is a Lagrange multiplier. For each value of $\\lambda$, repor the value of your optimized objective function and the number of components of $x$ that are zeros (ie. the number of constraints that are saturated at your solution vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
